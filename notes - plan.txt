## plan outline ##

Create model
Qwen3 (0.6B): Qwen/Qwen3-0.6B-Base (Recommended - Latest 2025 model with strong multilingual support)

Generate SPICE only
Output must be only the SPICE netlist

Use: https://huggingface.co/Qwen/Qwen3-0.6B-Base
Run locally on RTX3080

Repo folders: `prompts/`, `benchmark/`, `runs/`, `eval/`, `results/`, `figures/`
Create a run script


Design the benchmark
build 20+ SPICE prompts across categories
ToDo
    DC sweep,
    transient RC,
    AC filter,
    MOSFET inverter,
    diode rectifier,
    op-amp macro (optional),
    subcircuit usage,
    etc.

Define metrics
Metrics I will be measuring:
​1. Syntax validity pass rate
    Defined as a yes/no per case based on rules that I can check reliably.
    SPICE: the presence of .end, at least one element line, at least one analysis directive, no obviously malformed lines.
        “Pass” if the netlist meets basic SPICE structure:
        has .end
        has at least one element line (R, C, L, V, I, M, X, etc.)
        includes required analysis directive(s) for that case (.tran / .ac / .dc / .op)
        no obvious malformed tokens (you can keep this simple)

​2. Parameter exact-match for key fields (like regions, doping, electrodes, models)
    Works best if I define, per benchmark case, a small set of key-value targets to check
    (e.g., electrode names, doping type/level, model name, region material).
    Use exact match for strings/keywords; consider tolerance only if you allow numeric flexibility.
    Define required items like:
        specific component values (e.g., R1=1k, C1=10n)
        model name presence (e.g., .model NMOS ... or .include <lib>)
        sweep settings (e.g., .dc V1 0 5 0.1)

​3. Build a 'component and section coverage' score
    - (ask: 'does the output include the required parts for a valid TCAD input?')
    - treat it as a checklist-based score:
        coverage = found_required_items / total_required_items
    required components exist (e.g., must include 1 voltage source, 2 resistors, 1 MOSFET)
    required directives exist (.tran, .plot/.print, .model/.include, .end)
    Score = found / expected.

Next, write down how each metric will be computed - not super detailed

Build the evaluation harness - need to figure out how to do this
    * Run prompts --> capture outputs
    * Score outputs with metrics
    * Save results (JSON/CSV) for comparisons

Create the baseline prompt
    * One simple instruction prompt
    * Run baseline across benchmark and record baseline scores

Implement prompt technique variants
    * Structured output constraints
    * Few-shot examples
    * Critique-->revise pass
    * retrieval/context injection - may not have time for this one

Run experiments and log results
    * Keep runs reproducible (fixed seeds like 123)
    * Store prompts + outputs + scores per run

Analyze + prompting strategy
    * Compare deltas vs baseline
    * Collect representative successes/failures

Prompting techniques:
    P0: Baseline instruction prompt
    P1: Structured output rules + hard constraints
    P2: Few-shot (2–3 examples)
    P3: Critique --> revise (2-pass)

Prepare stuff to submit i.e. deliverables
    * Slides: method, benchmark, metrics, results
    * Report: concise writeup + plots/tables
    * README: how to run + reproduce experiments


## Assignment 4 - implementation ##

    Instruction Refinement - iterative prompt polishing

        We start from the P2 few-shot prompt and iteratively refine the wording of the instructions based on observed error patterns.

        Issues:
            Netlists missing output directives (.print, .plot, .meas).
            The model sometimes renames things (VIN ↔ V1, etc.).
            Device cases (diodes/MOSFETs) sometimes omit .model lines.
            Subcircuit cases can miss .ends or mis-name the subcircuit.

        Result:
            I refined the instructions to discourage renaming, enforce output directives, and require .model/.ends lines where relevant. This increased coverage from X to Y and exact-match from A to B.
            ** add a small table: P2 vs P4 (syntax_pass_rate, coverage, exact_match) **

            Role Prompting (P4) did not noticeably change performance compared to the original few-shot prompt (P2).

            This suggests that:
                The extra rules I added were either redundant with what the few-shot examples already taught, or
                They weren’t specific enough to change the model’s behavior on the hard cases (MOSFET, subcircuit, etc.).
                I tried to refine the instructions, and it didn’t help.

    Role Prompting - doesn't seem to difficult
        Issues:
        Result:
            P5 (Role Prompting) is roughly:
                - Better than P2 on all three metrics
                - Slightly better than P3 on syntax (+0.05) and exact-match (+0.004)
                - Slightly worse than P3 on coverage (0.3378 vs 0.3521)

            We expect this subtle effect: the role prompt nudges the model toward “acting like an ngspice expert,” which seems to help it produce syntactically valid netlists and match key lines more often, without completely changing coverage.

    Output Format Control - stronger structure
        Exactly the same as the original P2 few-shot prompt. This configuration didn’t change performance at all on the benchmark.
        This particular format constraint didn’t meaningfully affect the underlying netlist quality.

